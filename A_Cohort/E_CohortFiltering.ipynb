{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fb48f-a8d9-4e1d-aa67-1c06f8cfc605",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tableone import TableOne\n",
    "import getpass, re, json, sys\n",
    "from datetime import datetime, timedelta\n",
    "import pandas_gbq as pgbq\n",
    "# Load packages for Big Query \n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b1c636-63a9-427d-8b6a-52764c21776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define configurations for Big Query\n",
    "project_id = '' # Location of stride datalake\n",
    "client = bigquery.Client(project=project_id) # Set project to project_id\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = ''\n",
    "os.environ['GCLOUD_PROJECT'] = \"\" # specify environment\n",
    "db = \"\" # Define the database\n",
    "stanford_ds = \"\"\n",
    "yh_ds = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45872497-2edc-4898-8ce1-af698f907625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tableone import TableOne\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a135403-8588-44c6-8177-5d2cdcf0161b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_table(project_id, yh_ds, new_table_name, query):\n",
    "    table_id = f\"{project_id}.{yh_ds}.{new_table_name}\"\n",
    "    job_config = bigquery.QueryJobConfig(destination=table_id)\n",
    "    job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "    # Start the query, passing in the extra configuration.\n",
    "    query_job = client.query(query, job_config=job_config)  # Make an API request.\n",
    "    query_job.result()  # Wait for the job to complete.\n",
    "    print(\"Query results loaded to the table {}\".format(table_id))  \n",
    "def load_pgbq(project_id, yh_ds, table_name):\n",
    "    sql_query = f\"SELECT * FROM {project_id}.{yh_ds}.{table_name}\"\n",
    "    return_df = pgbq.read_gbq(sql_query, dialect=\"standard\")\n",
    "    print (f\"{project_id}.{yh_ds}.{table_name}\", \"is loaded\") \n",
    "    return return_df\n",
    "def upload_pgbq(project_id, yh_ds, table_name, df):\n",
    "    table_id = f\"{yh_ds}.{table_name}\"\n",
    "    pgbq.to_gbq(df, table_id, project_id=project_id)\n",
    "    print (\"dataframe\", df, \"is uploaded as\", f\"{project_id}.{yh_ds}.{table_name}\") \n",
    "def remove_table(project_id, yh_ds, table_name):\n",
    "    client = bigquery.Client()\n",
    "    table_id = f\"{project_id}.{yh_ds}.{table_name}\"\n",
    "    client.delete_table(table_id, not_found_ok=True)  # Make an API request.\n",
    "    print(\"Deleted table '{}'.\".format(table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48119bc-19e6-414e-a96c-2b5af98fdc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pat_table = \"dementia_pat_complete_cohort_07312024\"\n",
    "surgery_table = \"surgeries\"\n",
    "med_table = \"dementia_op_med_07312024\"\n",
    "diagnosis_table = \"shc_diagnosis\"\n",
    "\n",
    "sql_pneumonia_query = f\"\"\"\n",
    "WITH pneumonia_patients AS (\n",
    "    SELECT \n",
    "        d.pat_deid,\n",
    "        d.start_date\n",
    "    FROM `{project_id}.{stanford_ds}.{diagnosis_table}` d\n",
    "    WHERE LOWER(d.dx_name) LIKE '%pneumonia%'\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    p.pat_deid,\n",
    "    -- Check if pneumonia was diagnosed within 7 days before exposure start\n",
    "    CASE \n",
    "        WHEN DATE_DIFF(CAST(p.post_onset_exposure_start_time AS DATE), pneumonia_patients.start_date, DAY) BETWEEN 0 AND 7 THEN 1\n",
    "        ELSE 0\n",
    "    END AS pneumonia_before_7days_exposure,\n",
    "\n",
    "    -- Check if pneumonia was diagnosed after the exposure start\n",
    "    CASE \n",
    "        WHEN DATE_DIFF(pneumonia_patients.start_date, CAST(p.post_onset_exposure_start_time AS DATE), DAY) > 0 THEN 1\n",
    "        ELSE 0\n",
    "    END AS pneumonia_after_exposure,\n",
    "\n",
    "    -- Calculate the date difference between pneumonia diagnosis and exposure start\n",
    "    CASE\n",
    "        WHEN DATE_DIFF(pneumonia_patients.start_date, CAST(p.post_onset_exposure_start_time AS DATE), DAY) > 0 \n",
    "        THEN DATE_DIFF(pneumonia_patients.start_date, CAST(p.post_onset_exposure_start_time AS DATE), DAY)\n",
    "        ELSE NULL\n",
    "    END AS pneumonia_days_since_exposure\n",
    "FROM `{project_id}.{yh_ds}.{pat_table}` p\n",
    "LEFT JOIN pneumonia_patients \n",
    "    ON p.pat_deid = pneumonia_patients.pat_deid;\n",
    "\"\"\"\n",
    "\n",
    "pneumonia_df = pgbq.read_gbq(sql_pneumonia_query, dialect=\"standard\")\n",
    "\n",
    "sql_med_query = f\"\"\"\n",
    "SELECT \n",
    "    p.pat_deid,  -- Assuming patient_id is the unique identifier for each patient\n",
    "    MAX(m.opioid_strength_classification) AS opioid_strength_classification,\n",
    "    ARRAY_AGG(m.ingredient ORDER BY CAST(m.order_start_time AS DATE) ASC)[OFFSET(0)] AS ingredient\n",
    "FROM \n",
    "    `{project_id}.{yh_ds}.{pat_table}` p\n",
    "JOIN \n",
    "    `{project_id}.{yh_ds}.{med_table}` m\n",
    "    ON p.pat_deid = m.pat_deid  -- Join both tables on patient ID\n",
    "    AND CAST(p.post_onset_exposure_start_time AS DATE) = CAST(m.order_start_time AS DATE)  -- Ensure the date matches the first exposure date\n",
    "WHERE \n",
    "    m.order_start_time IS NOT NULL  -- Filter to make sure medication data is available\n",
    "GROUP BY \n",
    "    p.pat_deid, CAST(p.post_onset_exposure_start_time AS DATE);\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "med_df = pgbq.read_gbq(sql_med_query, dialect=\"standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a844b6-c138-4a5d-8c5b-fb5665222828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation dictionary: specify which columns to aggregate and how\n",
    "aggregation_dict = {\n",
    "    'pneumonia_before_7days_exposure': 'max',\n",
    "    'pneumonia_after_exposure': 'max',\n",
    "    'pneumonia_days_since_exposure': 'min'\n",
    "}\n",
    "\n",
    "# Perform aggregation by 'group' column\n",
    "pneumonia_ag_df = pneumonia_df.groupby('pat_deid').agg(aggregation_dict).reset_index()\n",
    "pneumonia_ag_df = pneumonia_ag_df.drop_duplicates(subset = 'pat_deid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d62072-1ba3-403b-a973-59809bb9561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "med_df = med_df.drop_duplicates(subset = 'pat_deid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627aae9-6ae5-4b1f-a319-332d810a5f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df = load_pgbq(project_id, yh_ds, pat_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9c2f48-46f9-4e65-888c-494846910080",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df = cohort_df.drop_duplicates(subset=['pat_deid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636cf9b4-f1cd-4d3f-b1bc-beb6b2cde0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df2 = pd.merge(cohort_df,pneumonia_ag_df, on='pat_deid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366de58-cc91-4b0d-8442-cda2255ec07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df3 = pd.merge(cohort_df2, med_df, on='pat_deid', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa18ce18-d298-45e7-a59c-5e78954a8fb9",
   "metadata": {},
   "source": [
    "### sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03922465-3598-4ac7-82cc-a54baf6f8ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_df3.exposure_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7eff71-99a5-4d86-b659-3f977f692292",
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check_columns = ['post_onset_post_opioid_death_days', 'post_onset_op_exposure_days', 'death_from_diagnosis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e4420e-4e7d-43ac-a637-09675b3ea88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot a histogram of the distribution\n",
    "cohort_df3['post_onset_post_opioid_death_days'].hist(bins=15, edgecolor='black')\n",
    "plt.title(\"Distribution of 'post_onset_post_opioid_death_days'\")\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17742329-57a2-4c3e-bef6-55ccce63eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'final_followup_date' with 'death_date' if available; otherwise, use 'last_encounter_date'\n",
    "cohort_df3['final_followup_date'] = cohort_df3['death_date'].fillna(cohort_df3['last_encounter_date'])\n",
    "cohort_df3['final_followup_date'] = pd.to_datetime(cohort_df3['final_followup_date'], errors='coerce')\n",
    "cohort_df3['post_onset_exposure_start_time'] = pd.to_datetime(cohort_df3['post_onset_exposure_start_time'], errors='coerce')\n",
    "# Ensure all date columns are tz-naive for consistent comparisons\n",
    "\n",
    "cohort_df3['final_followup_date'] = cohort_df3['final_followup_date'].dt.tz_localize(None)\n",
    "cohort_df3['post_onset_exposure_start_time'] = cohort_df3['post_onset_exposure_start_time'].dt.tz_localize(None)\n",
    "cohort_df3['diagnosis_date'] = cohort_df3['diagnosis_date'].dt.tz_localize(None)\n",
    "\n",
    "cohort_df3['post_onset_exposure_start_time'] = pd.to_datetime(cohort_df3['post_onset_exposure_start_time'])\n",
    "cohort_df3['follow_up_post_diagnosis_first_op_exposure'] = (cohort_df3['final_followup_date'] - cohort_df3['post_onset_exposure_start_time']).dt.days+1\n",
    "\n",
    "\n",
    "cohort_df3['death_date'] = cohort_df3['death_date'].dt.tz_localize(None)\n",
    "cohort_df3.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdca6f2-2642-4dde-bab0-40a13706a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Filter out rows where 'final_followup_date' is null\n",
    "cohort_df4 = cohort_df3[cohort_df3['final_followup_date'].notnull()]\n",
    "print(f\"Step 1: Sample size after filtering for non-null 'final_followup_date': {cohort_df4.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7088009-c755-4e59-9a06-dde8d689c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove rows where 'final_followup_date' is missing\n",
    "cohort_df4 = cohort_df3[cohort_df3['final_followup_date'].notnull()]\n",
    "print(f\"Step 1: Sample size after filtering for non-null 'final_followup_date': {cohort_df4.shape[0]}\")\n",
    "\n",
    "# Step 2: Keep only rows where 'final_followup_date' is on or after 'diagnosis_date'\n",
    "cohort_df5 = cohort_df4[cohort_df4['final_followup_date'] >= cohort_df4['diagnosis_date']]\n",
    "print(f\"Step 2: Sample size after ensuring 'final_followup_date' >= 'diagnosis_date': {cohort_df5.shape[0]}\")\n",
    "\n",
    "# Step 3: Retain rows where 'death_from_diagnosis' is either missing or has a non-negative value\n",
    "cohort_df6 = cohort_df5[\n",
    "    (cohort_df5['death_from_diagnosis'].isnull()) | (cohort_df5['death_from_diagnosis'] >= 0)\n",
    "]\n",
    "print(f\"Step 3: Sample size after filtering for null or non-negative 'death_from_diagnosis': {cohort_df6.shape[0]}\")\n",
    "\n",
    "# Step 3b: Count deaths within 14 days of opioid exposure in specific groups\n",
    "count2 = np.sum(\n",
    "    (cohort_df6['exposure_group'].isin(['consistent user', 'new user'])) & \n",
    "    (cohort_df6['post_onset_post_opioid_death_days'] <= 14)\n",
    ")\n",
    "print(f\"Step 3b: Number of deaths within 14 days of opioid exposure: {count2}\")\n",
    "\n",
    "# Step 4: Exclude rows with invalid follow-up durations for 'new user' or 'consistent user'\n",
    "cohort_df7 = cohort_df6[\n",
    "    ~(\n",
    "        cohort_df6['exposure_group'].isin(['new user', 'consistent user']) & \n",
    "        (cohort_df6['post_onset_exposure_start_time'] >= cohort_df6['final_followup_date'])\n",
    "    )\n",
    "]\n",
    "print(f\"Step 4: Sample size after excluding invalid follow-up durations: {cohort_df7.shape[0]}\")\n",
    "\n",
    "# Step 5: Exclude rows with invalid death dates for 'new user' or 'consistent user'\n",
    "cohort_df8 = cohort_df7[\n",
    "    ~(\n",
    "        cohort_df7['exposure_group'].isin(['new user', 'consistent user']) & \n",
    "        cohort_df7['death_date'].notna() & \n",
    "        (cohort_df7['death_date'] <= cohort_df7['post_onset_exposure_start_time'])\n",
    "    )\n",
    "]\n",
    "print(f\"Step 5: Sample size after excluding invalid death dates: {cohort_df8.shape[0]}\")\n",
    "\n",
    "# Step 6: Exclude rows with negative 'post_onset_post_opioid_death_days' for 'new user' or 'consistent user'\n",
    "cohort_df9 = cohort_df8[\n",
    "    ~(\n",
    "        cohort_df8['exposure_group'].isin(['new user', 'consistent user']) & \n",
    "        cohort_df8['post_onset_post_opioid_death_days'].notna() & \n",
    "        (cohort_df8['post_onset_post_opioid_death_days'] < 0)\n",
    "    )\n",
    "]\n",
    "print(f\"Step 6: Sample size after excluding negative 'post_onset_post_opioid_death_days': {cohort_df9.shape[0]}\")\n",
    "\n",
    "# Final: Count deaths within 14 days of opioid exposure in specific groups\n",
    "count3 = np.sum(\n",
    "    (cohort_df9['exposure_group'].isin(['consistent user', 'new user'])) & \n",
    "    (cohort_df9['post_onset_post_opioid_death_days'] <= 14)\n",
    ")\n",
    "print(f\"Final: Number of deaths within 14 days of opioid exposure: {count3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c49f33f-c545-48f3-bc3c-9c701309e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Keep rows where 'death_date' is either missing or on/after '2015-01-01'\n",
    "cohort_df10 = cohort_df9[\n",
    "    (cohort_df9['death_date'].isnull()) | \n",
    "    (cohort_df9['death_date'] >= '2015-01-01')\n",
    "]\n",
    "print(f\"Step 1: Sample size after filtering for valid 'death_date': {cohort_df10.shape[0]}\")\n",
    "\n",
    "# Step 2: Keep rows where 'final_followup_date' is on or after '2015-01-01'\n",
    "cohort_df11 = cohort_df10[cohort_df10['final_followup_date'] >= '2015-01-01']\n",
    "print(f\"Step 2: Sample size after filtering for 'final_followup_date' >= '2015-01-01': {cohort_df11.shape[0]}\")\n",
    "\n",
    "# Print the sample size of the final cohort after all filtering steps\n",
    "print(\"Sample size of cohort:\", cohort_df11.shape[0])\n",
    "\n",
    "# Count the number of deaths within 14 days of opioid exposure in 'consistent user' or 'new user' groups\n",
    "count4 = np.sum(\n",
    "    (cohort_df11['exposure_group'].isin(['consistent user', 'new user'])) & \n",
    "    (cohort_df11['post_onset_post_opioid_death_days'] <= 14)\n",
    ")\n",
    "\n",
    "# Print the final count of deaths\n",
    "print(f\"Final: Number of deaths within 14 days of opioid exposure: {count4}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e2deaa-54f3-4849-9193-f8428b1f4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the surgery data from the specified table\n",
    "df_surgery = load_pgbq(project_id, yh_ds, surgery_table)\n",
    "\n",
    "# Step 1: Merge the cohort DataFrame with the surgery DataFrame on 'pat_deid'\n",
    "df_surgery_combined = pd.merge(cohort_df11, df_surgery, on='pat_deid')\n",
    "\n",
    "# Step 2: Ensure 'surg_date' and 'death_date' columns are timezone-naive for comparison\n",
    "df_surgery_combined['surgery_date'] = df_surgery_combined['surg_date'].dt.tz_localize(None)\n",
    "df_surgery_combined['death_date'] = df_surgery_combined['death_date'].dt.tz_localize(None)\n",
    "\n",
    "# Step 3: Calculate the time difference (in days) between surgery date and death date\n",
    "df_surgery_combined['time_to_death_from_surgery'] = (\n",
    "    df_surgery_combined['death_date'] - df_surgery_combined['surgery_date']\n",
    ").dt.days\n",
    "\n",
    "# Step 4: Identify patients who died within 14 days of surgery\n",
    "patients_to_exclude = df_surgery_combined[\n",
    "    df_surgery_combined['time_to_death_from_surgery'] <= 14\n",
    "]['pat_deid']\n",
    "\n",
    "# Step 5: Exclude these patients from the original cohort DataFrame\n",
    "df_cohort_surgery_excluded = cohort_df11[\n",
    "    ~cohort_df11['pat_deid'].isin(patients_to_exclude)\n",
    "]\n",
    "\n",
    "print(f\"Final cohort size after excluding deaths within 14 days of surgery: {df_cohort_surgery_excluded.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38bc0fd-1a36-4005-b729-bb25a471250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Print sample size after excluding patients who died within 14 days of surgery\n",
    "print(\"Sample size of cohort after exclusion of patients who died within 14 days after surgery:\", df_cohort_surgery_excluded.shape[0])\n",
    "\n",
    "# Step 2: Filter for patients with continuity of care (more than 2 visits before and after dementia diagnosis)\n",
    "df_continued_care = df_cohort_surgery_excluded[\n",
    "    (df_cohort_surgery_excluded['visits_before'] > 2) & \n",
    "    (df_cohort_surgery_excluded['visits_after'] > 2)\n",
    "]\n",
    "print(\"Sample size of cohort who received continuity of care before and after dementia diagnosis:\", df_continued_care.shape[0])\n",
    "\n",
    "# Step 3: Exclude patients who received their first dementia diagnosis near the time of death (< 7 days)\n",
    "df_continued_care2 = df_continued_care[\n",
    "    ~((df_continued_care['death_date'] - df_continued_care['diagnosis_date']).dt.days < 7)\n",
    "]\n",
    "print(\"Sample size of cohort after exclusion of patients who received first dementia diagnosis at the time of death:\", df_continued_care2.shape[0])\n",
    "\n",
    "# Step 4: Count deaths within 14 days of opioid exposure in the filtered cohort\n",
    "death_count = np.sum(\n",
    "    (df_continued_care2['exposure_group'].isin(['consistent user', 'new user'])) & \n",
    "    (df_continued_care2['post_onset_post_opioid_death_days'] <= 14)\n",
    ")\n",
    "print(f\"Number of deaths within 14 days of opioid exposure in the filtered cohort: {death_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc58f5e-861c-4d67-b8cd-f97b33530127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a copy of the DataFrame to avoid modifying the original\n",
    "df_copy = df_continued_care2.copy()\n",
    "\n",
    "# Step 2: Ensure 'diagnosis_date' is in datetime format and remove any timezone info\n",
    "df_copy['diagnosis_date'] = pd.to_datetime(df_copy['diagnosis_date']).dt.tz_localize(None)\n",
    "\n",
    "# Step 3: Define the cutoff date (July 31, 2024) as a timezone-naive datetime object\n",
    "cutoff_date = pd.to_datetime('2024-07-31')\n",
    "\n",
    "# Step 4: Filter the cohort to include only patients diagnosed on or before the cutoff date\n",
    "df_final = df_copy[df_copy['diagnosis_date'] <= cutoff_date]\n",
    "\n",
    "# Step 5: Remove duplicate patients based on their unique identifier 'pat_deid'\n",
    "df_final = df_final.drop_duplicates(subset='pat_deid')\n",
    "\n",
    "# Step 6: Print the final sample size after filtering\n",
    "print(\"Sample size of cohort after exclusion of patients diagnosed after July 2024:\", df_final.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3700ce-6299-47b5-8f5f-e9aa7efadf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get long-term opioid use category \n",
    "df_final2 = df_final.copy()\n",
    "df_final2['pre_longterm_opioid'] = np.where(df_final2['pre_onset_duration'] >= 90, 1, 0)\n",
    "df_final2['post_longterm_opioid'] = np.where(df_final2['post_onset_duration'] >= 90, 1, 0)\n",
    "df_final2['pre_longterm_opioid_consistent'] = np.where((df_final2['pre_longterm_opioid'] == 1)&(df_final2['exposure_group'] == 'consistent user') , 1, 0)\n",
    "df_final2['post_longterm_opioid_consistent'] = np.where((df_final2['post_longterm_opioid'] == 1)&(df_final2['exposure_group'] == 'consistent user') , 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be2f41f-63ec-4d1f-943b-1829fefcb1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final2.exposure_group.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbc31a9-0a9d-427f-b254-bc430581c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final2.to_csv('final_cohort_07312024.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
